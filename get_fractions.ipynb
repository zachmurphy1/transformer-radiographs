{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc60bfd9-6213-4e5b-8dce-961615c05380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil\n",
    "import json\n",
    "import pickle\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c783a285-9b6b-45fa-8e7d-ddb5bbaae445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cis/home/zmurphy/code/data/results/final/DenseNet121_lr0.05_bs16_optSGD_wd0.0001_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nlbatch_do0.5_1624137117.pkl\n",
      "Accuracy: \t263122\t / \t358344 \t= 0.734\n",
      "Precision: \t19602\t / \t107220 \t= 0.183\n",
      "Recall: \t19602\t / \t27206 \t= 0.721\n",
      "F1: \t\t19602\t / \t67213 \t= 0.292\n",
      "/cis/home/zmurphy/code/data/results/final/DeiT_lr0.05_bs16_optSGD_wd1e-05_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nllayer_do0.0_1624113464.pkl\n",
      "Accuracy: \t258979\t / \t358344 \t= 0.723\n",
      "Precision: \t19267\t / \t110693 \t= 0.174\n",
      "Recall: \t19267\t / \t27206 \t= 0.708\n",
      "F1: \t\t19267\t / \t68950 \t= 0.279\n",
      "/cis/home/zmurphy/code/data/results/final/DeiT-Ti_lr0.01_bs16_optSGD_wd0.0_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nllayer_do0.0_1646947623.pkl\n",
      "Accuracy: \t254010\t / \t358344 \t= 0.709\n",
      "Precision: \t18972\t / \t115072 \t= 0.165\n",
      "Recall: \t18972\t / \t27206 \t= 0.697\n",
      "F1: \t\t18972\t / \t71139 \t= 0.267\n",
      "/cis/home/zmurphy/code/data/results/final/ResNet152_lr0.05_bs16_optSGD_wd1e-05_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nlbatch_do0.5_1647001594.pkl\n",
      "Accuracy: \t262615\t / \t358344 \t= 0.733\n",
      "Precision: \t19525\t / \t107573 \t= 0.182\n",
      "Recall: \t19525\t / \t27206 \t= 0.718\n",
      "F1: \t\t19525\t / \t67390 \t= 0.290\n",
      "/cis/home/zmurphy/code/data/results/final/EfficientNet_B7_lr0.05_bs16_optSGD_wd0.0001_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nlbatch_do0.5_1647035773.pkl\n",
      "Accuracy: \t263995\t / \t358344 \t= 0.737\n",
      "Precision: \t19720\t / \t106583 \t= 0.185\n",
      "Recall: \t19720\t / \t27206 \t= 0.725\n",
      "F1: \t\t19720\t / \t66894 \t= 0.295\n"
     ]
    }
   ],
   "source": [
    "# Parse args\n",
    "sys.argv = ['']\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--cfg-dir\", default='/cis/home/zmurphy/code/transformer-radiographs/cfg.json', type=str, help='')\n",
    "parser.add_argument(\"--scratch-dir\", default='/export/gaon1/data/zmurphy/transformer-cxr', type=str, help='')\n",
    "parser.add_argument(\"--results-dir\", default='/export/gaon1/data/zmurphy/transformer-cxr/results/final', type=str, help='')\n",
    "parser.add_argument(\"--to-analyze\", default='/export/gaon1/data/zmurphy/transformer-cxr/results/final/to_analyze_CXR100-Extended.json', type=str, help='')\n",
    "parser.add_argument(\"--dir-name\", default='CXR100_Extended', type=str, help='')\n",
    "parser.add_argument(\"--bootstrap-dir\", default='bootstrap_raw.pkl', type=str, help='')\n",
    "parser.add_argument(\"--plots\", default='y', type=str, help='')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Get cfg\n",
    "with open(args.cfg_dir.replace('~', os.path.expanduser('~')), 'r') as f:\n",
    "    cfg = json.load(f)\n",
    "cfg['data_dir'] = cfg['data_dir'].replace('~', os.path.expanduser('~'))\n",
    "\n",
    "# Parse args\n",
    "labels = cfg['labels_chexnet_14_standard']\n",
    "args.scratch_dir = args.scratch_dir.replace('~',os.path.expanduser('~'))\n",
    "args.results_dir = args.results_dir.replace('~',os.path.expanduser('~'))\n",
    "args.to_analyze = args.to_analyze.replace('~',os.path.expanduser('~'))\n",
    "args.dir_name = os.path.join(args.results_dir, args.dir_name)\n",
    "if os.path.exists(args.dir_name):\n",
    "  shutil.rmtree(args.dir_name)\n",
    "os.mkdir(args.dir_name)\n",
    "\n",
    "with open(args.to_analyze, 'r') as f:\n",
    "  to_analyze = json.load(f)\n",
    "  \n",
    "results = {}\n",
    "for t in to_analyze:\n",
    "  print(t['file'])\n",
    "  with open(t['file'], 'rb') as f:\n",
    "    m = pickle.load(f)\n",
    "    \n",
    "  sets = {}\n",
    "  for s in ['nihcxr14_test']:\n",
    "    y = np.array(m[s]['y'])\n",
    "    yhat = np.array(m[s]['yhat'])\n",
    "    name = '{}-{}'.format(t['name'], s)\n",
    "    \n",
    "    # Resample\n",
    "    metrics = {\n",
    "      'auc_weighted':[],\n",
    "      'precision_micro':[],\n",
    "      'recall_micro':[],\n",
    "      'f1_micro':[],\n",
    "      'accuracy_micro':[]\n",
    "    }\n",
    "    for l in labels:\n",
    "      metrics['auc_{}'.format(l)] = []\n",
    "      metrics['wt_{}'.format(l)] = []\n",
    "      metrics['precision_{}'.format(l)] = []\n",
    "      metrics['recall_{}'.format(l)] = []\n",
    "      metrics['f1_{}'.format(l)] = []\n",
    "      metrics['accuracy_{}'.format(l)] = []\n",
    "      metrics['thresh_{}'.format(l)] = []\n",
    "    for i in range(1):\n",
    "      # Resample\n",
    "      resampled_idxs = range(y.shape[0])\n",
    "      y_rs = y[resampled_idxs,:]\n",
    "      yhat_rs = yhat[resampled_idxs,:]\n",
    "        \n",
    "      # By label\n",
    "      \n",
    "      confusion_matrix_total = np.zeros((2,2))\n",
    "      for li, l in enumerate(labels):\n",
    "        ys_sub = np.array(y_rs)[:,li]\n",
    "        yhats_sub = np.array(yhat_rs)[:,li]\n",
    "        if (np.mean(ys_sub) > 0) and (np.mean(ys_sub) < 1):\n",
    "          # AUC\n",
    "          metrics['auc_{}'.format(l)].append(roc_auc_score(ys_sub, yhats_sub))\n",
    "          metrics['wt_{}'.format(l)].append(np.sum(ys_sub)/np.sum(y_rs))\n",
    "          \n",
    "          # Confusion matrix metricx\n",
    "          # Get optimal threshold\n",
    "          fpr, tpr, thresholds = roc_curve(ys_sub, yhats_sub, pos_label=1)\n",
    "          fnr = 1 - tpr\n",
    "          op_idx = np.nanargmin(np.absolute(((tpr) - (1-fpr))))\n",
    "          op_thresh = thresholds[op_idx]\n",
    "          metrics['thresh_{}'.format(l)].append(op_thresh)\n",
    "          # Confusion matrix\n",
    "          confusion_matrix = np.zeros((2,2))\n",
    "          for j in range(y_rs.shape[0]):\n",
    "            pred = 0\n",
    "            if yhat_rs[j,li] >= op_thresh:\n",
    "              pred = 1\n",
    "            confusion_matrix[pred, int(y_rs[j,li])] += 1\n",
    "\n",
    "          # Calculate confusion matrix metrics\n",
    "          metrics['precision_{}'.format(l)].append(confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[1,0]))\n",
    "          metrics['recall_{}'.format(l)].append(confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[0,1]))\n",
    "          metrics['f1_{}'.format(l)].append(2*metrics['precision_{}'.format(l)][-1]*metrics['recall_{}'.format(l)][-1]/(metrics['precision_{}'.format(l)][-1]+metrics['recall_{}'.format(l)][-1]))\n",
    "          metrics['accuracy_{}'.format(l)].append((confusion_matrix[0,0] + confusion_matrix[1,1]) / confusion_matrix.sum())\n",
    "          # Add to confusion matrix\n",
    "          confusion_matrix_total = np.add(confusion_matrix_total, confusion_matrix)\n",
    "      \n",
    "        else:\n",
    "          metrics['auc_{}'.format(l)].append(np.NaN)\n",
    "          metrics['wt_{}'.format(l)].append(np.NaN)\n",
    "          metrics['precision_{}'.format(l)].append(np.NaN)\n",
    "          metrics['recall_{}'.format(l)].append(np.NaN)\n",
    "          metrics['f1_{}'.format(l)].append(np.NaN)\n",
    "          metrics['accuracy_{}'.format(l)].append(np.NaN)\n",
    "          metrics['thresh_{}'.format(l)].append(np.NaN)\n",
    "\n",
    "    tn = confusion_matrix_total[0,0]\n",
    "    tp = confusion_matrix_total[1,1]\n",
    "    fn = confusion_matrix_total[0,1]\n",
    "    fp = confusion_matrix_total[1,0]\n",
    "    total = tp + tn + fn + fp\n",
    "    print('Accuracy: \\t{:.0f}\\t / \\t{:.0f} \\t= {:.3f}'.format(tp+tn, total, (tp+tn)/total))\n",
    "    print('Precision: \\t{:.0f}\\t / \\t{:.0f} \\t= {:.3f}'.format(tp, tp+fp, (tp)/(tp+fp)))\n",
    "    print('Recall: \\t{:.0f}\\t / \\t{:.0f} \\t= {:.3f}'.format(tp, tp+fn, (tp)/(tp+fn)))\n",
    "    print('F1: \\t\\t{:.0f}\\t / \\t{:.0f} \\t= {:.3f}'.format(tp, tp + 0.5*(fp + fn), tp/(tp + 0.5*(fp + fn))))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e6fab8-0162-451b-9eae-6b96036ae8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cis/home/zmurphy/code/data/mura_results/final/DenseNet121_lr0.005_bs16_optSGD_wd1e-05_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nlbatch_do0.5_1625271936.pkl\n",
      "Accuracy: \t6811\t / \t8393 \t= 0.812\n",
      "Precision: \t3090\t / \t3996 \t= 0.773\n",
      "Recall: \t3090\t / \t3766 \t= 0.820\n",
      "F1: \t\t3090\t / \t3881 \t= 0.796\n",
      "/cis/home/zmurphy/code/data/mura_results/final/DeiT_lr0.005_bs16_optSGD_wd1e-05_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nllayer_do0.0_1625279193.pkl\n",
      "Accuracy: \t6714\t / \t8393 \t= 0.800\n",
      "Precision: \t3026\t / \t3965 \t= 0.763\n",
      "Recall: \t3026\t / \t3766 \t= 0.804\n",
      "F1: \t\t3026\t / \t3866 \t= 0.783\n",
      "/cis/home/zmurphy/code/data/mura_results/final/DeiT-Ti_lr0.005_bs16_optSGD_wd0.0001_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nllayer_do0.0_1647737635.pkl\n",
      "Accuracy: \t6317\t / \t8393 \t= 0.753\n",
      "Precision: \t2934\t / \t4178 \t= 0.702\n",
      "Recall: \t2934\t / \t3766 \t= 0.779\n",
      "F1: \t\t2934\t / \t3972 \t= 0.739\n",
      "/cis/home/zmurphy/code/data/mura_results/final/ResNet152_lr0.005_bs16_optSGD_wd1e-05_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nlbatch_do0.0_1648573154.pkl\n",
      "Accuracy: \t6886\t / \t8393 \t= 0.820\n",
      "Precision: \t3124\t / \t3989 \t= 0.783\n",
      "Recall: \t3124\t / \t3766 \t= 0.830\n",
      "F1: \t\t3124\t / \t3878 \t= 0.806\n",
      "/cis/home/zmurphy/code/data/mura_results/final/EfficientNet_B7_lr0.005_bs16_optSGD_wd0.0001_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nlbatch_do0.5_1647778207.pkl\n",
      "Accuracy: \t6916\t / \t8393 \t= 0.824\n",
      "Precision: \t3077\t / \t3865 \t= 0.796\n",
      "Recall: \t3077\t / \t3766 \t= 0.817\n",
      "F1: \t\t3077\t / \t3816 \t= 0.806\n"
     ]
    }
   ],
   "source": [
    "# Parse args\n",
    "sys.argv = ['']\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--cfg-dir\", default='/cis/home/zmurphy/code/transformer-radiographs/cfg.json', type=str, help='')\n",
    "parser.add_argument(\"--scratch-dir\", default='/export/gaon1/data/zmurphy/transformer-mura', type=str, help='')\n",
    "parser.add_argument(\"--results-dir\", default='/export/gaon1/data/zmurphy/transformer-cxr/mura_results/final', type=str, help='')\n",
    "parser.add_argument(\"--to-analyze\", default='/export/gaon1/data/zmurphy/transformer-cxr/mura_results/final/to_analyze_MURA100-Extended.json', type=str, help='')\n",
    "parser.add_argument(\"--dir-name\", default='MURA100_Extended', type=str, help='')\n",
    "parser.add_argument(\"--bootstrap-dir\", default='bootstrap_raw.pkl', type=str, help='')\n",
    "parser.add_argument(\"--plots\", default='y', type=str, help='')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# Get cfg\n",
    "with open(args.cfg_dir.replace('~', os.path.expanduser('~')), 'r') as f:\n",
    "    cfg = json.load(f)\n",
    "cfg['data_dir'] = cfg['data_dir'].replace('~', os.path.expanduser('~'))\n",
    "\n",
    "# Parse args\n",
    "labels = cfg['labels_mura_standard'][1:]\n",
    "args.scratch_dir = args.scratch_dir.replace('~',os.path.expanduser('~'))\n",
    "args.results_dir = args.results_dir.replace('~',os.path.expanduser('~'))\n",
    "args.to_analyze = args.to_analyze.replace('~',os.path.expanduser('~'))\n",
    "args.dir_name = os.path.join(args.results_dir, args.dir_name)\n",
    "if os.path.exists(args.dir_name):\n",
    "  shutil.rmtree(args.dir_name)\n",
    "os.mkdir(args.dir_name)\n",
    "regions = ['shoulder','humerus','elbow', 'forearm','wrist','hand','finger']\n",
    "\n",
    "with open(args.to_analyze, 'r') as f:\n",
    "  to_analyze = json.load(f)\n",
    "\n",
    "# Set n resamples\n",
    "n = 1\n",
    "\n",
    "# Get bootstrapped metrics\n",
    "results = {}\n",
    "for t in to_analyze:\n",
    "  print(t['file'])\n",
    "  with open(t['file'], 'rb') as f:\n",
    "    m = pickle.load(f)\n",
    "    \n",
    "  sets = {}\n",
    "  for s in m.keys():\n",
    "    dat = pd.DataFrame().from_dict({'y':[int(x[1]) for x in m[s]['y']],\n",
    "                                   'yhat':[x[1] for x in m[s]['yhat']],\n",
    "                                   'region':m[s]['region'],\n",
    "                                   'study':m[s]['study']})\n",
    "\n",
    "    # Group by study, take study yhat as mean of image yhats\n",
    "    dat_grouped = dat.groupby('study').mean()\n",
    "    dat_region = dat[['study','region']].groupby('study').first()\n",
    "    dat_grouped['y'] = dat_grouped['y'].astype(int)\n",
    "    dat_grouped['region'] = dat_region['region']\n",
    "    \n",
    "    # Resample\n",
    "    metrics = {\n",
    "      'auc_weighted':[],\n",
    "      'precision_micro':[],\n",
    "      'recall_micro':[],\n",
    "      'f1_micro':[],\n",
    "      'accuracy_micro':[]\n",
    "    }\n",
    "    for l in regions:\n",
    "      metrics['auc_{}'.format(l)] = []\n",
    "      metrics['wt_{}'.format(l)] = []\n",
    "      metrics['precision_{}'.format(l)] = []\n",
    "      metrics['recall_{}'.format(l)] = []\n",
    "      metrics['f1_{}'.format(l)] = []\n",
    "      metrics['accuracy_{}'.format(l)] = []\n",
    "      metrics['thresh_{}'.format(l)] = []\n",
    "    for i in range(n):\n",
    "\n",
    "      # Resample\n",
    "      resampled_idxs = range(dat_grouped.shape[0])\n",
    "      dat_rs = dat_grouped.iloc[resampled_idxs]\n",
    "        \n",
    "      # By label\n",
    "      confusion_matrix_total = np.zeros((2,2))\n",
    "      for li, l in enumerate(regions):\n",
    "        dat_rs_sub = dat_rs[dat_rs['region']==l]\n",
    "        ys_sub = np.array(dat_rs_sub['y'])\n",
    "        yhats_sub = np.array(dat_rs_sub['yhat'])\n",
    "        if (np.mean(ys_sub) > 0) and (np.mean(ys_sub) < 1):\n",
    "          # AUC\n",
    "          metrics['auc_{}'.format(l)].append(roc_auc_score(ys_sub, yhats_sub))\n",
    "          metrics['wt_{}'.format(l)].append(np.sum(ys_sub)/np.sum(dat_rs['y']))\n",
    "          \n",
    "          # Confusion matrix metricx\n",
    "          # Get optimal threshold\n",
    "          fpr, tpr, thresholds = roc_curve(ys_sub, yhats_sub, pos_label=1)\n",
    "          fnr = 1 - tpr\n",
    "          op_idx = np.nanargmin(np.absolute(((tpr) - (1-fpr))))\n",
    "          op_thresh = thresholds[op_idx]\n",
    "          metrics['thresh_{}'.format(l)].append(op_thresh)\n",
    "          # Confusion matrix\n",
    "          confusion_matrix = np.zeros((2,2))\n",
    "          for j in range(dat_rs.shape[0]):\n",
    "            pred = 0\n",
    "            if dat_rs.iloc[j]['yhat'] >= op_thresh:\n",
    "              pred = 1\n",
    "            confusion_matrix[pred, int(dat_rs.iloc[j]['y'])] += 1\n",
    "\n",
    "          # Calculate confusion matrix metrics\n",
    "          metrics['precision_{}'.format(l)].append(confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[1,0]))\n",
    "          metrics['recall_{}'.format(l)].append(confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[0,1]))\n",
    "          metrics['f1_{}'.format(l)].append(2*metrics['precision_{}'.format(l)][-1]*metrics['recall_{}'.format(l)][-1]/(metrics['precision_{}'.format(l)][-1]+metrics['recall_{}'.format(l)][-1]))\n",
    "          metrics['accuracy_{}'.format(l)].append((confusion_matrix[0,0] + confusion_matrix[1,1]) / confusion_matrix.sum())\n",
    "          # Add to confusion matrix\n",
    "          confusion_matrix_total = np.add(confusion_matrix_total, confusion_matrix)\n",
    "      \n",
    "        else:\n",
    "          metrics['auc_{}'.format(l)].append(np.NaN)\n",
    "          metrics['wt_{}'.format(l)].append(np.NaN)\n",
    "          metrics['precision_{}'.format(l)].append(np.NaN)\n",
    "          metrics['recall_{}'.format(l)].append(np.NaN)\n",
    "          metrics['f1_{}'.format(l)].append(np.NaN)\n",
    "          metrics['accuracy_{}'.format(l)].append(np.NaN)\n",
    "          metrics['thresh_{}'.format(l)].append(np.NaN)\n",
    "          \n",
    "    tn = confusion_matrix_total[0,0]\n",
    "    tp = confusion_matrix_total[1,1]\n",
    "    fn = confusion_matrix_total[0,1]\n",
    "    fp = confusion_matrix_total[1,0]\n",
    "    total = tp + tn + fn + fp\n",
    "    print('Accuracy: \\t{:.0f}\\t / \\t{:.0f} \\t= {:.3f}'.format(tp+tn, total, (tp+tn)/total))\n",
    "    print('Precision: \\t{:.0f}\\t / \\t{:.0f} \\t= {:.3f}'.format(tp, tp+fp, (tp)/(tp+fp)))\n",
    "    print('Recall: \\t{:.0f}\\t / \\t{:.0f} \\t= {:.3f}'.format(tp, tp+fn, (tp)/(tp+fn)))\n",
    "    print('F1: \\t\\t{:.0f}\\t / \\t{:.0f} \\t= {:.3f}'.format(tp, tp + 0.5*(fp + fn), tp/(tp + 0.5*(fp + fn))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416574c-87fa-48b5-856a-6ae2bca4e807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
