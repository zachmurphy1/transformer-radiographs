{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e98c34-182c-4536-91e7-4ba83bde0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/cis/home/zmurphy/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/cis/home/zmurphy/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "Requirement already satisfied: opencv-python in /cis/home/zmurphy/.local/lib/python3.6/site-packages (4.5.5.62)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /cis/home/zmurphy/.local/lib/python3.6/site-packages (from opencv-python) (1.19.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/cis/home/zmurphy/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/cis/home/zmurphy/.local/lib/python3.6/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/cis/home/zmurphy/.local/lib/python3.6/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Imports\"\"\"\n",
    "import os, sys, shutil, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "!pip install opencv-python\n",
    "import cv2\n",
    "!pip install nopdb\n",
    "import nopdb\n",
    "\n",
    "# Custom\n",
    "import custom_modules as CXR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cb36478c-5a70-4725-8296-bb307ce72f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set GPU\"\"\"\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0' # <------ Be sure to set the right GPU!!!\n",
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c036a087-5631-4528-86e8-edf27d3836e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Config\"\"\"\n",
    "with open('/cis/home/zmurphy/code/transformer-radiographs/cfg.json'.replace('~', os.path.expanduser('~')), 'r') as f:\n",
    "    cfg = json.load(f)\n",
    "    \n",
    "model_args = {\n",
    "    'model_state': '/cis/home/zmurphy/code/data/results/final/DeiT_lr0.05_bs16_optSGD_wd1e-05_sch_step_pp3_bp5_trtrain_all.txt_vatest.txt_tfhflip_nllayer_do0.0_1624113464_model.pt',\n",
    "    'labels_set': 'chexnet-14-standard',\n",
    "    'labels': cfg['labels_chexnet_14_standard'],\n",
    "    'n_labels': len(cfg['labels_chexnet_14_standard']),\n",
    "    'batch_size': 16,\n",
    "    'data_dir': cfg['data_dir'],\n",
    "    'dataset': 'nihcxr14',\n",
    "    'test_file': 'test_correct.txt',\n",
    "    'use_parallel': 'y',\n",
    "    'num_workers': 12,\n",
    "    'img_size': 224,\n",
    "    'print_batches': False,\n",
    "    'scratch_dir':'/export/gaon1/data/zmurphy/transformer-cxr',\n",
    "    'results_dir':'/export/gaon1/data/zmurphy/transformer-cxr/results/final'\n",
    "}\n",
    "\n",
    "# ImageNet mean, std\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "1f51bdba-19f4-46f2-80df-0a11d9893392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in ./export/gaon1/data/zmurphy/transformer-cxr/results/facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Get model and set state\"\"\"\n",
    "torch.hub.set_dir('base_model_states')\n",
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "model.head = nn.Sequential(nn.Linear(in_features=768, out_features=14), nn.Sigmoid())\n",
    "model.load_state_dict(torch.load(model_args['model_state'], map_location=torch.device('cpu')))\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "\n",
    "# Model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "1119c816-e00f-4995-98d0-913b5aed27d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: starting load\n",
      "Using image path file\n",
      "Using no transforms\n",
      "Loaded 271 images\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load data\"\"\"\n",
    "dataset_root = os.path.join(model_args['data_dir'], model_args['dataset'])\n",
    "test_data = CXR.CXRDataset(images_list=os.path.join(dataset_root, model_args['test_file']),\n",
    "                            dataset=model_args['dataset'],\n",
    "                            images_dir=os.path.join(dataset_root, 'images'),\n",
    "                            image_paths=os.path.join(dataset_root, 'image_paths.txt'),\n",
    "                            labels_file=os.path.join(dataset_root, 'labels.csv'),\n",
    "                            labels=model_args['labels'],\n",
    "                            transform='none',\n",
    "                            op='test',\n",
    "                            img_size=model_args['img_size'])\n",
    "testLoader = DataLoader(test_data, batch_size=model_args['batch_size'],\n",
    "                         pin_memory=True, shuffle=True,\n",
    "                         num_workers=model_args['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "edcb01c2-5a85-4cc2-90fe-2dfa662b61b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get attention map\"\"\"\n",
    "def predict(x, model):\n",
    "  with torch.no_grad():\n",
    "    yhat = model(x)\n",
    "  return yhat\n",
    "\n",
    "def get_attention_map(x, model, layer_to_get=-1, cmap=cm.get_cmap('jet',256)):\n",
    "  # To GPU\n",
    "  x = x.to(device)\n",
    "  \n",
    "  # Get attention wts from each layer\n",
    "  attn_wts = []\n",
    "  for layer in range(12):\n",
    "    with nopdb.capture_call(model.blocks[layer].attn.forward) as attn_call:\n",
    "      yhat = predict(x,model)\n",
    "    attn_wts.append(attn_call.locals['attn'][0])\n",
    "  attn_wts = torch.stack(attn_wts).squeeze(1)\n",
    "  x = x.to('cpu')\n",
    "\n",
    "  # Get mean over attention heads for each layer\n",
    "  attn_wts_mean = torch.mean(attn_wts, 1)\n",
    "  attn_wts_mean = attn_wts_mean.to('cpu')\n",
    "\n",
    "  # Attention rollout\n",
    "  eye = torch.eye(attn_wts_mean.shape[-1])\n",
    "  attn_rollout = [0.5*attn_wts_mean[0,:,:] + 0.5*eye]\n",
    "  for layer in range(1,attn_wts_mean.shape[0]):\n",
    "    attn_rollout_layer = torch.matmul(0.5*attn_wts_mean[layer,:,:] + 0.5*eye, attn_rollout[layer-1])\n",
    "    attn_rollout.append(attn_rollout_layer)\n",
    "\n",
    "  # Get attention map for given layer\n",
    "  grid_size = int(np.sqrt(attn_rollout[layer_to_get].shape[-1]))\n",
    "  attn_map = attn_rollout[layer_to_get][0,1:].reshape(grid_size, grid_size).numpy() # Get matrix from top row without position token (first position)\n",
    "\n",
    "  # Scale values and resize\n",
    "  attn_map = attn_map - attn_map.min()\n",
    "  attn_map = attn_map/attn_map.max()\n",
    "  attn_map = cmap(cv2.resize(attn_map, dsize=(224, 224)),alpha = 1)\n",
    "  \n",
    "  return attn_map, yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8d332-a679-43ed-973f-07039ed4ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loop through input images\"\"\"\n",
    "target_dir='attn_maps'\n",
    "if os.path.exists(target_dir):\n",
    "  shutil.rmtree(target_dir)\n",
    "os.mkdir(target_dir)\n",
    "\n",
    "for x, y, file in testLoader:\n",
    "  # For each image in batch\n",
    "  for i in range(x.shape[0]):\n",
    "    \n",
    "    # Get attention map and yhat\n",
    "    attn_map, yhat = get_attention_map(x[i,:,:].unsqueeze(0),model)\n",
    "\n",
    "    # Reverse ImageNet mean & std for CXR\n",
    "    im = x[i,:,:]\n",
    "    im = im * std[:, None, None] + mean[:, None, None]\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    fig.patch.set_facecolor('white')\n",
    "    ax[0].imshow(im.numpy().transpose(1, 2, 0))\n",
    "    #ax[1].imshow(im.numpy().transpose(1, 2, 0))\n",
    "    ax[1].imshow(attn_map)\n",
    "    ax[0].axis('off')\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    # Get labels for image\n",
    "    labs = []\n",
    "    for l in range(14):\n",
    "      if y[i][l] == 1:\n",
    "        labs.append(model_args['labels'][l])\n",
    "    if len(labs) == 0:\n",
    "      plt.suptitle(file[i][file[i].rfind('/')+1:] + '\\n' + 'No findings', y=0.9)\n",
    "    else:\n",
    "      plt.suptitle(file[i][file[i].rfind('/')+1:] + '\\n' + ', '.join(labs), y=0.9)\n",
    "      \n",
    "    # Show plot\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(os.path.join(target_dir, file[i][file[i].rfind('/')+1:]))\n",
    "\n",
    "  # Only do one batch\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b61e884-fa03-4506-9c32-a2a4351b52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Check ImageNet model\"\"\"\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "torch.hub.set_dir('base_model_states')\n",
    "#model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True) # DeiT-B\n",
    "#model = torch.hub.load('facebookresearch/deit:main', 'deit_base_distilled_patch16_224', pretrained=True) # Distilled DeiT-B. Change [0,1:] to [0,2:] since distillation adds another token.\n",
    "#model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16') # DINO, performs best\n",
    "\n",
    "tfms = transforms.Compose([\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Resize((224,224), transforms.functional.InterpolationMode.BILINEAR),\n",
    "  transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "file_name = 'zebra2'\n",
    "img = Image.open('google_images/'+file_name+'.jpeg')\n",
    "img = tfms(img)\n",
    "\n",
    "model = model.to(device)\n",
    "attn_map, yhat = get_attention_map(img.unsqueeze(0),model, layer_to_get=-1)\n",
    "\n",
    "# Normalize image\n",
    "im = img\n",
    "im = im * std[:, None, None] + mean[:, None, None]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.patch.set_facecolor('white')\n",
    "ax[0].imshow(im.numpy().transpose(1, 2, 0))\n",
    "#ax[1].imshow(im.numpy().transpose(1, 2, 0))\n",
    "ax[1].imshow(attn_map)\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "\n",
    "# Show plot\n",
    "fig.tight_layout()\n",
    "plt.savefig(os.path.join('deit_distilled',file_name+'.png'))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06697b-9433-4882-b1aa-34e986de8c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
